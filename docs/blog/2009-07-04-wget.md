---
title: wget
date: 2009-07-04T15:40:00+08
tags:
  - CMD程式
---
# wget

這個軟體跟C++有什麼關係呢??  
因為它可以拿來抓網頁!  
只要再寫一個程式就可以分析網頁啦^^  
還支援登入功能喔  
下面是不同的文章  
我偷懶一點  
就用顏色區分吧

意見者：

[artist](http://panel.pixnet.cc/my/my?show=AE04150740) ( 初學者 5 級 )

發表時間：

2009-07-02 01:39:25

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AEbe0x1zcxm&date=2009-07-02 01:39:25&.crumb=.YVo.Wz9C6L) \]

我剛看了 wget 他好像是要在 linux 下才可以用 雖然很方便  
但是 現在不考慮用到 linux 呢 我如果理解錯誤麻煩告知喔 感激  
！！！謝摟

 [![](http://l.yimg.com/d/lib/ks/akp/i9/tw/l2_3232.gif)](http://panel.pixnet.cc/my/my?show=AE04150740) 002

意見者：

[artist](http://panel.pixnet.cc/my/my?show=AE04150740) ( 初學者 5 級 )

發表時間：

2009-07-02 02:20:54

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AEbe01xzcxm&date=2009-07-02 02:20:54&.crumb=.YVo.Wz9C6L) \]

我剛去看找資料 好像用 php的 curl 比較快

 [![](http://l.yimg.com/d/lib/ks/akp/i9/tw/l2_3232.gif)](http://panel.pixnet.cc/my/my?show=AE03201838) 003

意見者：

[DeeR](http://panel.pixnet.cc/my/my?show=AE03201838) ( 初學者 1 級 )

發表時間：

2009-07-02 09:59:10

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AE92918alm&date=2009-07-02 09:59:10&.crumb=.YVo.Wz9C6L) \]

wget 有 window 版呀

 [![](http://l.yimg.com/d/lib/ks/akp/i9/tw/l2_3232.gif)](http://panel.pixnet.cc/my/my?show=AE04150740) 004

意見者：

[artist](http://panel.pixnet.cc/my/my?show=AE04150740) ( 初學者 5 級 )

發表時間：

2009-07-02 14:38:14

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AEbe01zcjm&date=2009-07-02 14:38:14&.crumb=.YVo.Wz9C6L) \]

不好意思請問一下 我怎麼沒有看到有參數可以把整個網頁的文字(原始碼) 輸入到檔案中 不知是否我有遺漏掉是哪個參數可以做到這種功能 麻煩告知謝謝

 [![](http://l.yimg.com/d/lib/ks/akp/i9/tw/l2_3232.gif)](http://panel.pixnet.cc/my/my?show=AE03201838) 005

意見者：

[DeeR](http://panel.pixnet.cc/my/my?show=AE03201838) ( 初學者 1 級 )

發表時間：

2009-07-02 15:05:53

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AE92918vavm&date=2009-07-02 15:05:53&.crumb=.YVo.Wz9C6L) \]

我的用法是  
wget url -O out\_file\_name.txt  
  
加上 -q 是別出現其它的字來吵你。  
如果還要帳密。就加上  
\--user=...  
\--password=...

 [![](http://l.yimg.com/d/lib/ks/akp/i9/tw/l2_3232.gif)](http://panel.pixnet.cc/my/my?show=AE04150740) 006

意見者：

[artist](http://panel.pixnet.cc/my/my?show=AE04150740) ( 初學者 5 級 )

發表時間：

2009-07-02 17:19:36

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AEbe01izcim&date=2009-07-02 17:19:36&.crumb=.YVo.Wz9C6L) \]

okok 了 我之前不行 原來是我一直都打成 O- out\_file\_name.txt  
謝謝你提醒 我才知道 我打錯了  
不知他可不可以 擷取某些想知道的字串 的功能 如果有麻煩告知 謝謝喔！！！

 [![](http://l.yimg.com/d/lib/ks/akp/i9/tw/l2_3232.gif)](http://panel.pixnet.cc/my/my?show=AE03201838) 007

意見者：

[DeeR](http://panel.pixnet.cc/my/my?show=AE03201838) ( 初學者 1 級 )

發表時間：

2009-07-02 19:07:10

\[ [檢舉](http://panel.pixnet.cc/blog/accuse_write?type=c&qid=1609070108699&kid=AE92918awm&date=2009-07-02 19:07:10&.crumb=.YVo.Wz9C6L) \]

想要去切那些字串.不想寫程式.  
一個是用 grep.  
一個是用 awk  
  
grep 我記得 borland 的工具中有附.其它的工具我不知道了  
  
awk 在 windows 下有 gawk 不過用法得自己去看了.  
  
我的話.我會先用 grep 去切出我要的資料段.  
然後在那幾行自己去 parser.  
  
grep 大致會用到的功能還有一個參數 -v  
  
awk 就要你自己看 help 了. 它大到無法用三言兩言來交待. ^^

發信人: slime.bbs@tropic.med.kmu.edu.tw (轉出), 看板: Linux  
標 題: 【文件】介紹一個"砍站"軟體 wget  
發信站: 熱帶魚天堂 (Mon Sep 25 01:17:55 2000)  
轉信站: Cynix!netnews.hinet.net!spring!nsysu-news!news.kmu!TROPIC  
Origin: tropic.med.kmu.edu.tw  
  
砍站這個名詞的意思是說, 把一個站的東西一股腦兒全都抓下來,  
聽到這動作, 很多人可能會先想到 teleport pro 這套軟體,  
而現在介紹的這套, wget , 除了可以"砍站"之外,  
還可以支援續傳, 可以說 wget >= teleport pro + get right ,  
  
現在就先說明一下, wget 是 GNU 組織發展的軟體之一,  
有 Linux 與 Windows 版, 而且用法相同, 不需要註冊或破解,  
一般人的使用上是沒有限制的, 有點心動了嗎?  
  
那接著就來說明一下, wget 的用法, 因為是提供"砍站"的功能,  
所以 wget 並沒有互動式的介面, 完全是用文字模式處理,  
那怎麼知道要抓什麼呢? 就必須先用瀏覽器, 或 ftp 軟體確定,  
  
例如要抓高醫的網頁, 只要在文字模式, 下這樣的命令:  
  
wget <http://www.kmu.edu.tw/>  
看起來夠簡單吧, 一個小小的動作, 整個高醫網頁就抓下來了.  
不過這樣會抓到很多不相關的東西, 就可以用一些參數來設定,  
常用的參數如下:  
  
\-np 只抓該站內的資料, 因為網頁有超鏈結到其他網站的功能,  
加上 -np 參數, 就可以限制只抓該站內資料.  
  
\-m 是 mirror 的縮寫, 也就是將整個網站, 連同目錄結構都抓下來.  
  
\-A 只抓某些副檔名, 例如 -A html,htm 表示只抓網頁而不抓圖.  
  
\-b 丟到背景執行, 在 Windows 下可以讓 wget 不佔用 DOS 模式.  
  
\-c 續傳, 如果之前有抓到一半中斷的網站, 可以用這功能續傳,  
而且不需要網站支援續傳功能, wget 會自動從中斷的地方續傳.  
  
例如我可以下這樣的指令:  
wget -A jpeg,jpg -b -c -m -np <http://www..idv.tw/>  
將該網站的 jpeg 圖片全部抓下來.  
如何? 夠方便吧? 而且 wget 不只可以抓網頁, 還可以抓 ftp  
wget <ftp://ftp.nsysu.edu.tw/>  
需要帳號密碼的站台:wget <ftp://user:password@ftp.individual.com.tw/>  
需要特殊埠號(Port)的站台:  
wget <ftp://user:password@ftp.individual.com.tw:6667/>  
  
只要一個命令, 就可以將整個站的資料全部抓下來,  
wget 的功能就是這麼強大, 相對的, 對於頻寬也就有很大的要求.  
所以儘可能的, 設定正確的代理伺服器(proxy)來降低網路負載:  
  
像高醫內可以用 set http\_proxy=http://proxy.kmu.edu.tw:3128/  
使用 hinet 可以用 set http\_proxy=http://www.hinet.net:80/  
使用 seed.net 可以用 set <http://proxy=http://ksproxy.seed.net.tw:8080/>  
  
至於軟體的取得, 可以從這裡找到 Windows 版的 wget :  
[ftp://ftp.ntust.edu.tw/WinNT/Winsoc...win-1\_5\_3\_1.zip](ftp://ftp.ntust.edu.tw/WinNT/Winsock/FTP/wgetwin-1_5_3_1.zip)  
解開之後, 直接執行 wget 就可以了, 如果您想把網路頻寬塞滿,  
相信 wget 不會讓你失望的.
